<!doctype html>
<html lang="pt-br">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Guia de Referência — MPI com Python (mpi4py)</title>
  <style>
    :root{
      --bg:#0b0f14;
      --panel:#121821;
      --ink:#e6eef6;
      --muted:#9bb0c3;
      --accent:#58a6ff;
      --code:#0f1720;
      --border:#1f2937;
    }
    *{box-sizing:border-box}
    html,body{height:100%}
    body{
      margin:0;
      font:16px/1.6 system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,Cantarell,Noto Sans,sans-serif;
      color:var(--ink);
      background:linear-gradient(180deg,#0b0f14,#0a0d12);
      padding:2rem 0;
    }
    .wrap{max-width:1000px;margin:0 auto;padding:0 1.2rem}
    header{
      background:linear-gradient(180deg, rgba(88,166,255,.12), transparent);
      border:1px solid var(--border);
      border-radius:16px;
      padding:1.2rem 1.4rem;
      margin-bottom:1.2rem;
    }
    h1{font-size:2rem;margin:.2rem 0 0}
    h2{margin-top:2rem;font-size:1.5rem;border-bottom:1px solid var(--border);padding-bottom:.3rem}
    h3{margin-top:1.2rem;font-size:1.1rem}
    p{margin:.7rem 0}
    a{color:var(--accent);text-decoration:none}
    a:hover{text-decoration:underline}
    code,kbd,samp{background:var(--code);border:1px solid var(--border);padding:.15rem .35rem;border-radius:6px}
    pre{
      background:var(--code);
      border:1px solid var(--border);
      border-radius:12px;
      padding:1rem;
      overflow:auto;
    }
    .note{
      background:rgba(88,166,255,.08);
      border:1px solid var(--border);
      padding:.8rem 1rem;
      border-radius:12px;
      margin:1rem 0;
    }
    ul{margin:.5rem 0 .5rem 1.25rem}
    li{margin:.25rem 0}
    .tree{font-family:ui-monospace, SFMono-Regular, Menlo, Consolas, "Liberation Mono", monospace;}
    .tag{display:inline-block;padding:.1rem .5rem;border:1px solid var(--border);border-radius:999px;font-size:.8rem;color:var(--muted);margin-left:.4rem}
    footer{color:var(--muted);font-size:.9rem;margin-top:2rem}
  </style>
</head>
<body>
  <div class="wrap">
    <header>
      <div class="tag">Documentação</div>
      <h1>Guia de Referência — MPI com Python (mpi4py)</h1>
      <p>Este repositório reúne <strong>vários exemplos didáticos</strong> de programação paralela com <strong>MPI + Python (mpi4py)</strong>. Cada exemplo foca uma <strong>técnica</strong> de comunicação/coordenação (coletivas, ponto-a-ponto, modos não bloqueantes, envio bufferizado, tipos derivados, grupos/comunicadores, etc.) com analogias simples para ensino.</p>
      <div class="note"><strong>Onde estão os códigos?</strong> Todos os scripts citados abaixo estão na pasta <code>&lt;SOURCE&gt;</code>.</div>
    </header>

    <section id="pre">
      <h2>Pré-requisitos</h2>
      <ul>
        <li>Python 3.8+</li>
        <li><code>mpi4py</code></li>
        <li>Uma implementação de MPI (OpenMPI/MPICH no Linux/macOS; MS-MPI/MPICH no Windows)</li>
      </ul>
      <pre><code># mpi4py
pip install mpi4py</code></pre>
      <p><em>Dica:</em> em Linux/macOS use <code>mpiexec</code>/<code>mpirun</code>. Em Windows, prefira <code>mpiexec</code>.</p>
    </section>

    <section id="execucao">
      <h2>Como executar os exemplos</h2>
      <pre><code>mpiexec -n &lt;NUM_PROC&gt; python &lt;SOURCE&gt;/&lt;arquivo&gt;.py
# ou
mpiexec -n &lt;NUM_PROC&gt; python3 &lt;SOURCE&gt;/&lt;arquivo&gt;.py</code></pre>
      <p>Quando indicado, alguns exemplos exigem <strong>número de processos potência de 2</strong> (2, 4, 8, 16, …).</p>
    </section>

    <section id="sumario">
      <h2>Sumário de exemplos</h2>

      <h3>1) Coletivas de redução e agregação</h3>
      <ul>
        <li><strong>mpi_pi_criancas.py</strong> — <em>Allreduce (soma) para aproximar π</em><br>
          Cada processo integra um pedaço e <strong>soma global</strong> com <code>allreduce</code>.<br>
          <strong>Técnicas:</strong> <code>COMM_WORLD.allreduce</code>, particionamento por <em>stride</em>.<br>
          <strong>Rodar:</strong> <code>mpiexec -n 4 python &lt;SOURCE&gt;/mpi_pi_criancas.py</code></li>

        <li><strong>mpi_media_desvio_allreduce.py</strong> — <em>média e desvio padrão globais</em><br>
          Primeiro <code>allreduce</code> para a <strong>soma global</strong> (média), depois outro <code>allreduce</code> para a <strong>soma das diferenças²</strong> (desvio padrão).<br>
          <strong>Técnicas:</strong> duas passagens de <code>allreduce</code> (SUM).</li>

        <li><strong>mpi_medias_allgather.py</strong> — <em>troca de médias locais com Allgather</em><br>
          Cada processo calcula sua média local e todos trocam com <code>allgather</code> para computar a <strong>média da turma</strong>.<br>
          <strong>Técnicas:</strong> <code>COMM_WORLD.allgather</code>.</li>

        <li><strong>mpi_gather_criancas.py</strong> — <em>coleta de vetores no líder</em><br>
          O líder junta blocos iguais vindos de todos.<br>
          <strong>Técnicas:</strong> <code>COMM_WORLD.gather</code> (objeto Python), concatenação no líder.</li>
      </ul>

      <h3>2) Ponto-a-ponto (bloqueante, não bloqueante, síncrono, bufferizado)</h3>
      <ul>
        <li><strong>mpi_aleatorio_criancas.py</strong> — <em>Send/Recv + descoberta do tamanho recebido</em><br>
          O rank 0 envia uma quantidade <strong>aleatória</strong> de inteiros; o rank 1 recebe com buffer maior e usa <code>Status.Get_count</code> para saber <strong>quantos</strong> chegaram (<code>ANY_SOURCE</code>/<code>ANY_TAG</code>).<br>
          <strong>Técnicas:</strong> <code>Send/Recv</code>, <code>Status.Get_count</code>, <code>MPI.ANY_SOURCE</code>, <code>MPI.ANY_TAG</code>.</li>

        <li><strong>mpi_isend_criancas.py</strong> — <em>não bloqueante com Isend/Irecv</em><br>
          Trocas em <strong>recursive doubling</strong>; posta <code>Isend/Irecv</code>, depois <code>Waitall</code>.<br>
          <strong>Técnicas:</strong> <code>Isend/Irecv</code>, <code>Request.Waitall</code>, padrão <strong>recursive doubling</strong> (par em <code>rank ± i</code>).</li>

        <li><strong>mpi_sincrona_criancas.py</strong> — <em>envio síncrono com Ssend</em><br>
          Em cada rodada, metade envia primeiro e a outra metade recebe primeiro, evitando deadlock.<br>
          <strong>Técnicas:</strong> <code>Ssend/Recv</code>, alternância de ordem, <strong>recursive doubling</strong>.</li>

        <li><strong>mpi_bsend_criancas.py</strong> — <em>envio bufferizado com Bsend</em><br>
          Anexa um <strong>buffer</strong> (<code>Attach_buffer</code>), calcula tamanho de mensagem e faz trocas <strong>recursive doubling</strong> guardando o <strong>máximo</strong> elemento a elemento.<br>
          <strong>Técnicas:</strong> <code>Attach_buffer/Detach_buffer</code>, <code>Bsend/Recv</code>, cálculo de <code>BSEND_OVERHEAD</code>.</li>
      </ul>

      <h3>3) Tipos derivados (dados não contíguos / structs)</h3>
      <ul>
        <li><strong>mpi_bcast_coluna_vector_fix.py</strong> — <em>broadcast de uma coluna da matriz</em><br>
          No root, a coluna é não contígua em memória. Criamos um <strong>hvector</strong> e ancoramos no endereço <strong>absoluto</strong> do 1º elemento com <code>Create_struct</code> + <code>MPI.BOTTOM</code>. Demais processos recebem em vetor contíguo.<br>
          <strong>Técnicas:</strong> <code>Datatype.Create_hvector</code>, <code>Datatype.Create_struct</code>, <code>MPI.BOTTOM</code>, <code>Get_address</code>.</li>

        <li><strong>mpi_particulas_criancas.py</strong> — <em>struct “Partícula” + broadcast</em><br>
          Definimos <code>numpy.dtype</code> e um tipo MPI equivalente com <code>Create_struct</code>; o root preenche e difunde para todos.<br>
          <strong>Técnicas:</strong> <code>Datatype.Create_struct</code>, <code>Bcast</code>, compatibilização <code>numpy.dtype</code> ↔ tipo MPI.</li>

        <li><strong>mpi_particulas_grafico.py</strong> — <em>broadcast + transformação local + gather + gráfico</em><br>
          Após o broadcast, cada rank aplica um deslocamento e o root faz <code>Gather</code> para plotar (x × y) por processo.<br>
          <strong>Técnicas:</strong> <code>Bcast</code>, <code>Gather</code>, visualização com <code>matplotlib</code>.</li>
      </ul>

      <h3>4) “Quem tem o máximo e onde?” (MAXLOC emulado)</h3>
      <ul>
        <li><strong>mpi_maxloc_criancas.py</strong> — <em>emulação de MPI_MAXLOC</em><br>
          Para cada posição, faz <code>Allreduce(MAX)</code> no valor e, dentre quem empatou, <code>Reduce(MIN)</code> do rank (quem não empatou manda “∞”).<br>
          <strong>Técnicas:</strong> <code>Allreduce(MAX)</code> + <code>Reduce(MIN)</code> sobre candidatos.</li>
      </ul>

      <h3>5) Barreiras e difusão simples</h3>
      <ul>
        <li><strong>mpi_barreira_criancas.py</strong> — <em>barreira (“portão do parquinho”)</em><br>
          Ninguém avança até todos chegarem; o rank 0 “atrasado” espera teclado/tempo.<br>
          <strong>Técnicas:</strong> <code>Barrier</code>, uso de <code>Wtime/Wtick</code> (opcional).</li>

        <li><strong>mpi_broadcast_criancas.py</strong> — <em>broadcast de um valor escalar</em><br>
          O líder lê um inteiro e difunde para todos (versão simples via <code>bcast</code> de objeto Python).<br>
          <strong>Técnicas:</strong> <code>COMM_WORLD.bcast</code> (objeto), alternativa com <code>Bcast</code> + <code>numpy</code>.</li>
      </ul>

      <h3>6) Grupos e comunicadores</h3>
      <ul>
        <li><strong>mpi_grupos_criancas.py</strong> — <em>união de grupos arbitrários e novo comunicador</em><br>
          Criamos dois grupos “A” e “B” e depois a união (em mpi4py: <code>MPI.Group.Union(g1, g2)</code>). O novo comunicador contém os membros da união; cada participante ganha um novo rank.<br>
          <strong>Técnicas:</strong> <code>Get_group</code>, <code>Group.Incl</code>, <code>MPI.Group.Union</code>, <code>Comm.Create</code>, <code>Group.Get_rank</code>.</li>

        <li><strong>mpi_grupos_meia_turma.py</strong> — <em>divisão em metades fixas (8 processos)</em><br>
          Metade baixa <code>[0..3]</code> e metade alta <code>[4..7]</code>; cada grupo tem seu comunicador e faz <code>Allreduce(SUM)</code> dos ranks antigos.<br>
          <strong>Técnicas:</strong> <code>Group.Incl</code>, <code>Comm.Create</code>, <code>allreduce</code> intra-comunicador.</li>

        <li><strong>mpi_grupos_meia_turma_flex.py</strong> / <strong>mpi_grupos_meia_turma_stats.py</strong> — <em>divisão em metades para qualquer N ≥ 2</em><br>
          Particionamento em “Baixo” e “Cima” (o de cima pode ter 1 a mais se N ímpar), criação do comunicador e estatísticas do time: membros, tamanho, soma e média.<br>
          <strong>Técnicas:</strong> <code>Get_group</code>, <code>Group.Incl</code>, <code>Comm.Create</code>, <code>gather</code> de membros, <code>allreduce</code> da soma, cálculo de média no líder.</li>
      </ul>

      <h3>7) “Cartão de identidade” do job MPI</h3>
      <ul>
        <li><strong>mpi_funcoes_criancas.py</strong> — <em>versão do MPI, rank/size, nome da máquina, timers</em><br>
          Mostra versão/subversão, rank/size, hostname e mede tempo com <code>Wtime</code> (precisão <code>Wtick</code>).<br>
          <strong>Técnicas:</strong> <code>Get_version</code>, <code>Get_processor_name</code>, <code>Wtime/Wtick</code>.</li>

        <li><strong>mpi_funcoes_criancas_gather.py</strong> — <em>resumo agregado no líder</em><br>
          Cada processo envia seu “cartão” e o líder imprime tabela ordenada por rank.<br>
          <strong>Técnicas:</strong> <code>gather</code> de dicionários/objetos Python e formatação no líder.</li>
      </ul>
    </section>

    <section id="padroes">
      <h2>Padrões que aparecem com frequência</h2>
      <ul>
        <li><strong>Recursive Doubling</strong> (trocas em distâncias 1, 2, 4, …) — usado nos exemplos de redução via ponto-a-ponto: <code>mpi_isend_criancas.py</code>, <code>mpi_sincrona_criancas.py</code>, <code>mpi_bsend_criancas.py</code>.</li>
        <li><strong>Tipos derivados</strong><br>
          <em>Strided</em> (não contíguo): <code>Create_hvector</code> + <code>Create_struct</code> + <code>MPI.BOTTOM</code> — ver <code>mpi_bcast_coluna_vector_fix.py</code>.<br>
          <em>Structs</em> (campos mistos): <code>Create_struct</code> compatível com <code>numpy.dtype</code> — ver <code>mpi_particulas_criancas.py</code>.
        </li>
        <li><strong>Coletivas essenciais</strong> — <code>bcast/Bcast</code>, <code>gather/allgather</code>, <code>reduce/allreduce</code>, <code>Barrier</code>.</li>
        <li><strong>Status e contagem recebida</strong> — <code>Status.Get_count</code> para saber quantos elementos chegaram — ver <code>mpi_aleatorio_criancas.py</code>.</li>
      </ul>
    </section>

    <section id="dicas">
      <h2>Dicas e soluções de problemas</h2>
      <ul>
        <li><strong>Potência de 2:</strong> alguns exemplos assumem <code>-n</code> como 2, 4, 8, …</li>
        <li><strong>Bufferizado (Bsend):</strong> anexe um buffer suficientemente grande (<code>Attach_buffer</code>) e lembre de <code>Detach_buffer()</code>; em mpi4py, <code>Detach_buffer()</code> retorna só o buffer.</li>
        <li><strong>Não contíguo (vector/hvector):</strong> o lado root pode usar endereços absolutos com <code>MPI.BOTTOM</code> para evitar erros de “ndarray não contíguo”.</li>
        <li><strong>Nomes de métodos:</strong> em mpi4py use <code>Get_rank()</code> (e não <code>Get_Rank()</code>), etc.</li>
        <li><strong>ANY_SOURCE/ANY_TAG:</strong> use <code>MPI.ANY_SOURCE/MPI.ANY_TAG</code> com <code>Recv</code> quando quiser flexibilidade (e cheque <code>Status</code>).</li>
      </ul>
    </section>

    <section id="org">
      <h2>Organização</h2>
      <pre class="tree"><code>&lt;SOURCE&gt;/
  mpi_pi_criancas.py
  mpi_media_desvio_allreduce.py
  mpi_medias_allgather.py
  mpi_gather_criancas.py
  mpi_aleatorio_criancas.py
  mpi_isend_criancas.py
  mpi_sincrona_criancas.py
  mpi_bsend_criancas.py
  mpi_bcast_coluna_vector_fix.py
  mpi_particulas_criancas.py
  mpi_particulas_grafico.py
  mpi_maxloc_criancas.py
  mpi_barreira_criancas.py
  mpi_broadcast_criancas.py
  mpi_funcoes_criancas.py
  mpi_funcoes_criancas_gather.py
  mpi_grupos_criancas.py
  mpi_grupos_meia_turma.py
  mpi_grupos_meia_turma_flex.py
  mpi_grupos_meia_turma_stats.py</code></pre>
    </section>

    <footer>
      <p>Este guia foi montado a partir de exemplos e explicações didáticas produzidas durante nossa conversa, consolidando técnicas clássicas de MPI no contexto de <strong>mpi4py</strong>.</p>
    </footer>
  </div>
</body>
</html>
